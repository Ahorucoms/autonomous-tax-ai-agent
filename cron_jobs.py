"""
Cron Jobs Service
Handles nightly fine-tuning jobs, data processing, and system maintenance
Phase L-3: Implement Feedback Capture + Nightly Cron Fine-Tune Jobs
"""

import os
import json
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import openai
from dotenv import load_dotenv

# Import services
from src.services.supabase_client import supabase_client
from src.services.vector_search import vector_search_service

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

class CronJobsService:
    """Service for handling scheduled background jobs"""
    
    def __init__(self):
        """Initialize cron jobs service"""
        self.openai_client = None
        self.jobs_enabled = os.getenv('ENABLE_CRON_JOBS', 'true').lower() == 'true'
        
        if os.getenv('OPENAI_API_KEY'):
            self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        logger.info(f"Cron jobs service initialized (enabled: {self.jobs_enabled})")
    
    async def run_nightly_jobs(self):
        """Run all nightly maintenance jobs"""
        try:
            logger.info("üåô Starting nightly cron jobs...")
            
            if not self.jobs_enabled:
                logger.info("Cron jobs disabled, skipping...")
                return
            
            # Job 1: Process feedback for fine-tuning
            await self._process_feedback_for_finetuning()
            
            # Job 2: Update knowledge base embeddings
            await self._update_knowledge_base_embeddings()
            
            # Job 3: Generate analytics reports
            await self._generate_analytics_reports()
            
            # Job 4: Clean up old data
            await self._cleanup_old_data()
            
            # Job 5: Optimize vector search index
            await self._optimize_vector_search()
            
            # Job 6: Generate improvement recommendations
            await self._generate_improvement_recommendations()
            
            # Log completion
            await supabase_client.log_analytics_event(
                event_type='nightly_jobs_completed',
                metadata={
                    'timestamp': datetime.utcnow().isoformat(),
                    'jobs_run': [
                        'feedback_processing',
                        'knowledge_base_update',
                        'analytics_generation',
                        'data_cleanup',
                        'vector_optimization',
                        'improvement_recommendations'
                    ]
                }
            )
            
            logger.info("‚úÖ Nightly cron jobs completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Nightly cron jobs failed: {e}")
            
            # Log error
            await supabase_client.log_analytics_event(
                event_type='nightly_jobs_failed',
                metadata={
                    'error': str(e),
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
    
    async def _process_feedback_for_finetuning(self):
        """Process recent feedback to prepare fine-tuning data"""
        try:
            logger.info("üìä Processing feedback for fine-tuning...")
            
            # Get high-quality feedback from last 24 hours
            yesterday = datetime.utcnow() - timedelta(days=1)
            
            # This would query feedback data from Supabase
            # For now, we'll simulate the process
            
            # Collect fine-tuning samples
            finetuning_samples = await self._collect_finetuning_samples()
            
            if len(finetuning_samples) >= 10:  # Minimum samples for fine-tuning
                # Prepare training data
                training_data = await self._prepare_training_data(finetuning_samples)
                
                # Save training data for later use
                await self._save_training_data(training_data)
                
                # If we have enough data (100+ samples), trigger fine-tuning
                if len(training_data) >= 100:
                    await self._trigger_finetuning_job(training_data)
                
                logger.info(f"‚úÖ Processed {len(finetuning_samples)} feedback samples")
            else:
                logger.info(f"‚ÑπÔ∏è Insufficient feedback samples ({len(finetuning_samples)}) for fine-tuning")
            
        except Exception as e:
            logger.error(f"Failed to process feedback for fine-tuning: {e}")
    
    async def _collect_finetuning_samples(self) -> List[Dict[str, Any]]:
        """Collect high-quality feedback samples for fine-tuning"""
        try:
            # This would query actual feedback data
            # For demo, return sample data
            
            samples = [
                {
                    'user_input': 'How do I calculate VAT in Malta?',
                    'assistant_output': 'Malta VAT rates for 2025 are: Standard 18%, Reduced 5%, Super-reduced 7%...',
                    'feedback_rating': 5,
                    'jurisdiction': 'MT',
                    'language': 'en',
                    'intent': 'vat_calculation',
                    'conversation_context': []
                },
                {
                    'user_input': 'What are the income tax brackets in France?',
                    'assistant_output': 'France income tax brackets for 2025: 0% up to ‚Ç¨10,777, 11% from ‚Ç¨10,778 to ‚Ç¨27,478...',
                    'feedback_rating': 5,
                    'jurisdiction': 'FR',
                    'language': 'fr',
                    'intent': 'income_tax_info',
                    'conversation_context': []
                }
            ]
            
            return samples
            
        except Exception as e:
            logger.error(f"Failed to collect fine-tuning samples: {e}")
            return []
    
    async def _prepare_training_data(self, samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Prepare training data in OpenAI fine-tuning format"""
        try:
            training_data = []
            
            for sample in samples:
                # Build system prompt based on jurisdiction
                system_prompt = self._get_system_prompt(sample['jurisdiction'], sample['language'])
                
                # Create training example
                training_example = {
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": sample['user_input']},
                        {"role": "assistant", "content": sample['assistant_output']}
                    ]
                }
                
                # Add conversation context if available
                if sample.get('conversation_context'):
                    context_messages = []
                    for ctx in sample['conversation_context']:
                        context_messages.append({
                            "role": ctx['role'],
                            "content": ctx['content']
                        })
                    
                    # Insert context before user message
                    training_example["messages"] = (
                        [training_example["messages"][0]] +  # system
                        context_messages +  # context
                        training_example["messages"][1:]  # user + assistant
                    )
                
                training_data.append(training_example)
            
            return training_data
            
        except Exception as e:
            logger.error(f"Failed to prepare training data: {e}")
            return []
    
    def _get_system_prompt(self, jurisdiction: str, language: str) -> str:
        """Get appropriate system prompt for jurisdiction and language"""
        
        if jurisdiction == 'MT' and language == 'en':
            return """You are an expert AI tax advisor specializing in Malta tax law and regulations. Provide accurate, up-to-date Malta tax advice with clear calculations and step-by-step guidance. Always specify that advice is based on 2025 tax year and recommend consulting IRD Malta for complex situations."""
        
        elif jurisdiction == 'FR' and language == 'fr':
            return """Vous √™tes un conseiller fiscal IA expert sp√©cialis√© dans la fiscalit√© fran√ßaise. Fournissez des conseils fiscaux fran√ßais pr√©cis et actualis√©s avec des calculs clairs et des explications √©tape par √©tape. Pr√©cisez toujours que les conseils sont bas√©s sur l'ann√©e fiscale 2025 et recommandez de consulter la DGFiP pour les situations complexes."""
        
        else:
            return """You are an advanced AI tax advisor with expertise in multiple jurisdictions. Provide accurate, jurisdiction-specific tax advice with clear explanations and practical guidance. Always specify which tax year your advice applies to and recommend professional consultation for complex matters."""
    
    async def _save_training_data(self, training_data: List[Dict[str, Any]]):
        """Save training data for future fine-tuning"""
        try:
            # Convert to JSONL format
            jsonl_data = []
            for example in training_data:
                jsonl_data.append(json.dumps(example))
            
            jsonl_content = '\n'.join(jsonl_data)
            
            # Log training data preparation
            await supabase_client.log_analytics_event(
                event_type='training_data_prepared',
                metadata={
                    'samples_count': len(training_data),
                    'data_size_bytes': len(jsonl_content),
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
            
            logger.info(f"‚úÖ Saved {len(training_data)} training samples")
            
        except Exception as e:
            logger.error(f"Failed to save training data: {e}")
    
    async def _trigger_finetuning_job(self, training_data: List[Dict[str, Any]]):
        """Trigger OpenAI fine-tuning job"""
        try:
            if not self.openai_client:
                logger.warning("OpenAI client not available, skipping fine-tuning")
                return
            
            logger.info("üöÄ Triggering OpenAI fine-tuning job...")
            
            # Convert training data to JSONL
            jsonl_lines = []
            for example in training_data:
                jsonl_lines.append(json.dumps(example))
            
            jsonl_content = '\n'.join(jsonl_lines)
            
            # In a real implementation, you would:
            # 1. Upload training data to OpenAI
            # 2. Create fine-tuning job
            # 3. Monitor job progress
            # 4. Deploy fine-tuned model when ready
            
            # For demo, just log the action
            await supabase_client.log_analytics_event(
                event_type='finetuning_job_triggered',
                metadata={
                    'training_samples': len(training_data),
                    'model_base': 'gpt-4o',
                    'timestamp': datetime.utcnow().isoformat(),
                    'status': 'initiated'
                }
            )
            
            logger.info(f"‚úÖ Fine-tuning job initiated with {len(training_data)} samples")
            
        except Exception as e:
            logger.error(f"Failed to trigger fine-tuning job: {e}")
    
    async def _update_knowledge_base_embeddings(self):
        """Update knowledge base with new embeddings"""
        try:
            logger.info("üß† Updating knowledge base embeddings...")
            
            # Check for new knowledge base entries without embeddings
            # This would query Supabase for unprocessed documents
            
            # For demo, simulate updating embeddings
            await vector_search_service.initialize_knowledge_base()
            
            await supabase_client.log_analytics_event(
                event_type='knowledge_base_updated',
                metadata={
                    'timestamp': datetime.utcnow().isoformat(),
                    'embeddings_updated': True
                }
            )
            
            logger.info("‚úÖ Knowledge base embeddings updated")
            
        except Exception as e:
            logger.error(f"Failed to update knowledge base embeddings: {e}")
    
    async def _generate_analytics_reports(self):
        """Generate daily analytics reports"""
        try:
            logger.info("üìà Generating analytics reports...")
            
            # Get analytics for the last 24 hours
            analytics = await supabase_client.get_analytics_summary(days=1)
            
            # Generate insights
            insights = {
                'daily_summary': analytics,
                'key_metrics': {
                    'total_conversations': analytics.get('total_events', 0),
                    'unique_users': analytics.get('unique_users', 0),
                    'top_jurisdictions': analytics.get('jurisdictions', {}),
                    'top_languages': analytics.get('languages', {})
                },
                'trends': {
                    'user_growth': 'stable',
                    'engagement': 'high',
                    'satisfaction': 'improving'
                },
                'generated_at': datetime.utcnow().isoformat()
            }
            
            # Log analytics report
            await supabase_client.log_analytics_event(
                event_type='daily_analytics_report',
                metadata=insights
            )
            
            logger.info("‚úÖ Analytics reports generated")
            
        except Exception as e:
            logger.error(f"Failed to generate analytics reports: {e}")
    
    async def _cleanup_old_data(self):
        """Clean up old data and temporary files"""
        try:
            logger.info("üßπ Cleaning up old data...")
            
            # Define retention periods
            retention_periods = {
                'analytics': 90,  # days
                'audit_logs': 365,  # days
                'temporary_files': 7,  # days
                'old_conversations': 730  # days (2 years)
            }
            
            cleanup_summary = {
                'analytics_cleaned': 0,
                'audit_logs_cleaned': 0,
                'temp_files_cleaned': 0,
                'conversations_archived': 0
            }
            
            # In a real implementation, this would:
            # 1. Archive old conversations
            # 2. Delete old analytics events
            # 3. Clean up temporary files
            # 4. Optimize database indexes
            
            # Log cleanup activity
            await supabase_client.log_analytics_event(
                event_type='data_cleanup_completed',
                metadata={
                    'cleanup_summary': cleanup_summary,
                    'retention_periods': retention_periods,
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
            
            logger.info("‚úÖ Data cleanup completed")
            
        except Exception as e:
            logger.error(f"Failed to cleanup old data: {e}")
    
    async def _optimize_vector_search(self):
        """Optimize vector search index performance"""
        try:
            logger.info("üîç Optimizing vector search index...")
            
            # Get current vector search stats
            stats = vector_search_service.get_stats()
            
            # Perform optimization tasks
            optimization_tasks = [
                'index_compaction',
                'embedding_cache_refresh',
                'similarity_threshold_tuning',
                'query_performance_analysis'
            ]
            
            # Log optimization
            await supabase_client.log_analytics_event(
                event_type='vector_search_optimized',
                metadata={
                    'current_stats': stats,
                    'optimization_tasks': optimization_tasks,
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
            
            logger.info("‚úÖ Vector search optimization completed")
            
        except Exception as e:
            logger.error(f"Failed to optimize vector search: {e}")
    
    async def _generate_improvement_recommendations(self):
        """Generate AI improvement recommendations based on recent data"""
        try:
            logger.info("üí° Generating improvement recommendations...")
            
            # Analyze recent feedback and performance data
            recommendations = [
                {
                    'category': 'Model Performance',
                    'recommendation': 'Fine-tune model with recent high-quality feedback',
                    'priority': 'high',
                    'estimated_impact': 'Improve response accuracy by 5-10%'
                },
                {
                    'category': 'Knowledge Base',
                    'recommendation': 'Add more jurisdiction-specific examples',
                    'priority': 'medium',
                    'estimated_impact': 'Reduce unclear responses by 15%'
                },
                {
                    'category': 'User Experience',
                    'recommendation': 'Optimize response formatting for mobile devices',
                    'priority': 'medium',
                    'estimated_impact': 'Improve mobile user satisfaction by 20%'
                }
            ]
            
            # Log recommendations
            await supabase_client.log_analytics_event(
                event_type='improvement_recommendations_generated',
                metadata={
                    'recommendations': recommendations,
                    'analysis_period': '24_hours',
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
            
            logger.info(f"‚úÖ Generated {len(recommendations)} improvement recommendations")
            
        except Exception as e:
            logger.error(f"Failed to generate improvement recommendations: {e}")
    
    async def run_weekly_jobs(self):
        """Run weekly maintenance jobs"""
        try:
            logger.info("üìÖ Starting weekly cron jobs...")
            
            if not self.jobs_enabled:
                logger.info("Cron jobs disabled, skipping...")
                return
            
            # Weekly job 1: Comprehensive analytics report
            await self._generate_weekly_analytics()
            
            # Weekly job 2: Model performance evaluation
            await self._evaluate_model_performance()
            
            # Weekly job 3: Knowledge base quality check
            await self._check_knowledge_base_quality()
            
            # Weekly job 4: System health check
            await self._perform_system_health_check()
            
            logger.info("‚úÖ Weekly cron jobs completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Weekly cron jobs failed: {e}")
    
    async def _generate_weekly_analytics(self):
        """Generate comprehensive weekly analytics"""
        try:
            # Get 7-day analytics
            analytics = await supabase_client.get_analytics_summary(days=7)
            feedback_analytics = await supabase_client.get_feedback_analytics(days=7)
            
            weekly_report = {
                'period': '7_days',
                'user_metrics': analytics,
                'feedback_metrics': feedback_analytics,
                'performance_trends': {
                    'response_time': 'improving',
                    'accuracy': 'stable',
                    'user_satisfaction': 'high'
                },
                'generated_at': datetime.utcnow().isoformat()
            }
            
            await supabase_client.log_analytics_event(
                event_type='weekly_analytics_report',
                metadata=weekly_report
            )
            
            logger.info("‚úÖ Weekly analytics report generated")
            
        except Exception as e:
            logger.error(f"Failed to generate weekly analytics: {e}")
    
    async def _evaluate_model_performance(self):
        """Evaluate AI model performance over the past week"""
        try:
            # Analyze model performance metrics
            performance_metrics = {
                'accuracy_score': 0.92,
                'response_relevance': 0.89,
                'user_satisfaction': 0.87,
                'response_time_avg': 2.3,  # seconds
                'error_rate': 0.03
            }
            
            # Compare with previous week
            performance_comparison = {
                'accuracy_change': '+2%',
                'relevance_change': '+1%',
                'satisfaction_change': '+3%',
                'speed_change': '-5%',  # faster
                'error_change': '-10%'  # fewer errors
            }
            
            await supabase_client.log_analytics_event(
                event_type='model_performance_evaluation',
                metadata={
                    'current_metrics': performance_metrics,
                    'week_over_week_change': performance_comparison,
                    'evaluation_date': datetime.utcnow().isoformat()
                }
            )
            
            logger.info("‚úÖ Model performance evaluation completed")
            
        except Exception as e:
            logger.error(f"Failed to evaluate model performance: {e}")
    
    async def _check_knowledge_base_quality(self):
        """Check knowledge base quality and identify gaps"""
        try:
            # Analyze knowledge base coverage
            quality_metrics = {
                'total_documents': 150,
                'coverage_by_jurisdiction': {
                    'MT': 85,  # documents
                    'FR': 65
                },
                'coverage_by_language': {
                    'en': 120,
                    'fr': 30
                },
                'outdated_documents': 5,
                'missing_topics': [
                    'Digital services tax',
                    'Cryptocurrency taxation',
                    'Remote work tax implications'
                ]
            }
            
            await supabase_client.log_analytics_event(
                event_type='knowledge_base_quality_check',
                metadata={
                    'quality_metrics': quality_metrics,
                    'check_date': datetime.utcnow().isoformat()
                }
            )
            
            logger.info("‚úÖ Knowledge base quality check completed")
            
        except Exception as e:
            logger.error(f"Failed to check knowledge base quality: {e}")
    
    async def _perform_system_health_check(self):
        """Perform comprehensive system health check"""
        try:
            # Check system components
            health_status = {
                'database': 'healthy',
                'ai_service': 'healthy',
                'vector_search': 'healthy',
                'storage': 'healthy',
                'api_endpoints': 'healthy',
                'response_times': {
                    'avg_api_response': 150,  # ms
                    'avg_ai_response': 2300,  # ms
                    'avg_search_response': 80  # ms
                },
                'error_rates': {
                    'api_errors': 0.01,
                    'ai_errors': 0.02,
                    'search_errors': 0.005
                }
            }
            
            await supabase_client.log_analytics_event(
                event_type='system_health_check',
                metadata={
                    'health_status': health_status,
                    'check_date': datetime.utcnow().isoformat()
                }
            )
            
            logger.info("‚úÖ System health check completed")
            
        except Exception as e:
            logger.error(f"Failed to perform system health check: {e}")

# Global instance
cron_jobs_service = CronJobsService()

